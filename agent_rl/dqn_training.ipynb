{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Training\n",
    "Training a DQN agent using custom gym environment and keras-rl2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import MaxBoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), \"valid\") / w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Environmet of Tetris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = PacManEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Score: 6\n",
      "Episode: 2, Score: 12\n",
      "Episode: 3, Score: 10\n",
      "Episode: 4, Score: 6\n",
      "Episode: 5, Score: 15\n",
      "Episode: 6, Score: 3\n",
      "Episode: 7, Score: 3\n",
      "Episode: 8, Score: 29\n",
      "Episode: 9, Score: 2\n",
      "Episode: 10, Score: 11\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = np.random.randint(0, 5)\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(\"Episode: {}, Score: {}\".format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nural Network and Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    return tf.keras.Sequential([\n",
    "    # 1 state, 20 rows, 10 cols, 3 matricies: locked, falling and next figures\n",
    "    layers.Input(shape=(1, 20, 10, 3)),\n",
    "    layers.Reshape(target_shape=(20, 10, 3)),\n",
    "    \n",
    "    layers.Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\"),\n",
    "    layers.Activation(\"relu\"),\n",
    "    layers.MaxPool2D(),\n",
    "    \n",
    "    layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\"),\n",
    "    layers.Activation(\"relu\"),\n",
    "    layers.MaxPool2D(),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(32),\n",
    "    layers.Activation(\"relu\"),\n",
    "    \n",
    "    layers.Dense(16),\n",
    "    layers.Activation(\"relu\"),\n",
    "    \n",
    "    layers.Dense(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_5 (Reshape)         (None, 20, 10, 3)         0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 20, 10, 32)        896       \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 20, 10, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 10, 5, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 10, 5, 128)        36992     \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 10, 5, 128)        0         \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 5, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 64)                81984     \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,997\n",
      "Trainable params: 120,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 16, 4), 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.observation_space.shape, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model):\n",
    "    policy = MaxBoltzmannQPolicy(eps=0.8)\n",
    "    memory = SequentialMemory(limit=2048, window_length=1)\n",
    "    dqn = DQNAgent(model=model, \n",
    "        memory=memory,\n",
    "        policy=policy,\n",
    "        nb_actions=4,\n",
    "        gamma=0.99,\n",
    "        nb_steps_warmup=256,\n",
    "        batch_size=64,\n",
    "        target_model_update=0.1,\n",
    "        enable_double_dqn=True,\n",
    "        enable_dueling_network=True)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model)\n",
    "dqn.compile(tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0), metrics=[\"mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    }
   ],
   "source": [
    "#dqn.load_weights(\"\")\n",
    "\n",
    "#from keras import backend as K\n",
    "#print(K.eval(dqn.model.optimizer.learning_rate))\n",
    "#K.set_value(dqn.model.optimizer.learning_rate, 0.001)\n",
    "#print(K.eval(dqn.model.optimizer.learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 40000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "1000/1000 [==============================] - 33s 33ms/step - reward: 0.4110\n",
      "43 episodes - episode_reward: 9.535 [2.000, 33.000] - loss: 2.405 - mean_squared_error: 98.972 - mean_q: 11.039\n",
      "\n",
      "Interval 2 (1000 steps performed)\n",
      "1000/1000 [==============================] - 41s 41ms/step - reward: 0.4220\n",
      "42 episodes - episode_reward: 10.048 [2.000, 43.000] - loss: 2.369 - mean_squared_error: 103.279 - mean_q: 11.151\n",
      "\n",
      "Interval 3 (2000 steps performed)\n",
      "1000/1000 [==============================] - 42s 42ms/step - reward: 0.3970 1s - \n",
      "43 episodes - episode_reward: 9.163 [1.000, 30.000] - loss: 2.282 - mean_squared_error: 114.543 - mean_q: 11.416\n",
      "\n",
      "Interval 4 (3000 steps performed)\n",
      "1000/1000 [==============================] - 40s 40ms/step - reward: 0.4490\n",
      "41 episodes - episode_reward: 10.585 [2.000, 30.000] - loss: 2.417 - mean_squared_error: 125.440 - mean_q: 11.907\n",
      "\n",
      "Interval 5 (4000 steps performed)\n",
      "1000/1000 [==============================] - 41s 41ms/step - reward: 0.4650\n",
      "44 episodes - episode_reward: 10.705 [2.000, 37.000] - loss: 2.677 - mean_squared_error: 124.935 - mean_q: 11.996\n",
      "\n",
      "Interval 6 (5000 steps performed)\n",
      "1000/1000 [==============================] - 42s 42ms/step - reward: 0.4110\n",
      "39 episodes - episode_reward: 10.641 [2.000, 37.000] - loss: 2.800 - mean_squared_error: 140.816 - mean_q: 13.180\n",
      "\n",
      "Interval 7 (6000 steps performed)\n",
      "1000/1000 [==============================] - 39s 39ms/step - reward: 0.4530\n",
      "50 episodes - episode_reward: 9.100 [1.000, 26.000] - loss: 2.607 - mean_squared_error: 133.489 - mean_q: 12.688\n",
      "\n",
      "Interval 8 (7000 steps performed)\n",
      "1000/1000 [==============================] - 36s 36ms/step - reward: 0.4490\n",
      "51 episodes - episode_reward: 8.922 [2.000, 32.000] - loss: 2.484 - mean_squared_error: 107.704 - mean_q: 10.991\n",
      "\n",
      "Interval 9 (8000 steps performed)\n",
      "1000/1000 [==============================] - 40s 40ms/step - reward: 0.4660\n",
      "62 episodes - episode_reward: 7.468 [2.000, 24.000] - loss: 2.280 - mean_squared_error: 88.977 - mean_q: 10.070\n",
      "\n",
      "Interval 10 (9000 steps performed)\n",
      "1000/1000 [==============================] - 41s 41ms/step - reward: 0.3970\n",
      "46 episodes - episode_reward: 8.674 [1.000, 23.000] - loss: 1.895 - mean_squared_error: 69.443 - mean_q: 8.871\n",
      "\n",
      "Interval 11 (10000 steps performed)\n",
      "1000/1000 [==============================] - 37s 37ms/step - reward: 0.4020\n",
      "38 episodes - episode_reward: 10.553 [2.000, 32.000] - loss: 1.519 - mean_squared_error: 66.128 - mean_q: 8.936\n",
      "\n",
      "Interval 12 (11000 steps performed)\n",
      "1000/1000 [==============================] - 37s 37ms/step - reward: 0.4670\n",
      "48 episodes - episode_reward: 9.583 [2.000, 28.000] - loss: 1.585 - mean_squared_error: 76.732 - mean_q: 9.719\n",
      "\n",
      "Interval 13 (12000 steps performed)\n",
      "1000/1000 [==============================] - 35s 35ms/step - reward: 0.4710\n",
      "49 episodes - episode_reward: 9.633 [2.000, 36.000] - loss: 2.034 - mean_squared_error: 91.783 - mean_q: 10.407\n",
      "\n",
      "Interval 14 (13000 steps performed)\n",
      "1000/1000 [==============================] - 36s 36ms/step - reward: 0.4720\n",
      "55 episodes - episode_reward: 8.691 [1.000, 31.000] - loss: 2.260 - mean_squared_error: 91.163 - mean_q: 10.211\n",
      "\n",
      "Interval 15 (14000 steps performed)\n",
      "1000/1000 [==============================] - 38s 38ms/step - reward: 0.4360\n",
      "45 episodes - episode_reward: 9.356 [3.000, 28.000] - loss: 1.920 - mean_squared_error: 78.596 - mean_q: 9.551\n",
      "\n",
      "Interval 16 (15000 steps performed)\n",
      "1000/1000 [==============================] - 34s 34ms/step - reward: 0.4090\n",
      "51 episodes - episode_reward: 8.216 [1.000, 31.000] - loss: 1.898 - mean_squared_error: 75.615 - mean_q: 9.410\n",
      "\n",
      "Interval 17 (16000 steps performed)\n",
      "1000/1000 [==============================] - 33s 33ms/step - reward: 0.4570\n",
      "46 episodes - episode_reward: 9.804 [2.000, 33.000] - loss: 1.752 - mean_squared_error: 64.797 - mean_q: 8.827\n",
      "\n",
      "Interval 18 (17000 steps performed)\n",
      "1000/1000 [==============================] - 36s 36ms/step - reward: 0.3970\n",
      "42 episodes - episode_reward: 9.452 [3.000, 27.000] - loss: 1.928 - mean_squared_error: 83.122 - mean_q: 10.103\n",
      "\n",
      "Interval 19 (18000 steps performed)\n",
      "1000/1000 [==============================] - 34s 34ms/step - reward: 0.4420\n",
      "48 episodes - episode_reward: 9.479 [2.000, 30.000] - loss: 2.241 - mean_squared_error: 96.106 - mean_q: 10.540\n",
      "\n",
      "Interval 20 (19000 steps performed)\n",
      "1000/1000 [==============================] - 34s 34ms/step - reward: 0.4190\n",
      "43 episodes - episode_reward: 9.349 [2.000, 34.000] - loss: 9.982 - mean_squared_error: 195.087 - mean_q: 11.531\n",
      "\n",
      "Interval 21 (20000 steps performed)\n",
      "1000/1000 [==============================] - 35s 35ms/step - reward: 0.4470\n",
      "50 episodes - episode_reward: 9.180 [2.000, 25.000] - loss: 3.278 - mean_squared_error: 104.804 - mean_q: 10.946\n",
      "\n",
      "Interval 22 (21000 steps performed)\n",
      "1000/1000 [==============================] - 35s 35ms/step - reward: 0.4830\n",
      "44 episodes - episode_reward: 10.864 [1.000, 42.000] - loss: 1.981 - mean_squared_error: 74.883 - mean_q: 9.653\n",
      "\n",
      "Interval 23 (22000 steps performed)\n",
      "1000/1000 [==============================] - 35s 35ms/step - reward: 0.4140\n",
      "42 episodes - episode_reward: 9.738 [2.000, 27.000] - loss: 2.026 - mean_squared_error: 81.339 - mean_q: 10.032\n",
      "\n",
      "Interval 24 (23000 steps performed)\n",
      "1000/1000 [==============================] - 34s 34ms/step - reward: 0.4480\n",
      "43 episodes - episode_reward: 10.628 [2.000, 37.000] - loss: 2.212 - mean_squared_error: 99.102 - mean_q: 11.113\n",
      "\n",
      "Interval 25 (24000 steps performed)\n",
      "1000/1000 [==============================] - 34s 34ms/step - reward: 0.4230\n",
      "49 episodes - episode_reward: 8.694 [2.000, 25.000] - loss: 2.144 - mean_squared_error: 94.083 - mean_q: 10.717\n",
      "\n",
      "Interval 26 (25000 steps performed)\n",
      "1000/1000 [==============================] - 34s 34ms/step - reward: 0.4450\n",
      "65 episodes - episode_reward: 6.815 [1.000, 27.000] - loss: 1.526 - mean_squared_error: 63.694 - mean_q: 8.825\n",
      "\n",
      "Interval 27 (26000 steps performed)\n",
      "1000/1000 [==============================] - 34s 34ms/step - reward: 0.4250\n",
      "42 episodes - episode_reward: 10.048 [2.000, 29.000] - loss: 1.278 - mean_squared_error: 49.823 - mean_q: 7.707\n",
      "\n",
      "Interval 28 (27000 steps performed)\n",
      "1000/1000 [==============================] - 36s 36ms/step - reward: 0.4110\n",
      "38 episodes - episode_reward: 11.026 [2.000, 31.000] - loss: 1.463 - mean_squared_error: 65.784 - mean_q: 8.842\n",
      "\n",
      "Interval 29 (28000 steps performed)\n",
      "1000/1000 [==============================] - 40s 40ms/step - reward: 0.4420\n",
      "44 episodes - episode_reward: 9.977 [1.000, 22.000] - loss: 1.619 - mean_squared_error: 83.955 - mean_q: 10.029\n",
      "\n",
      "Interval 30 (29000 steps performed)\n",
      "1000/1000 [==============================] - 37s 37ms/step - reward: 0.4240 0s - re\n",
      "49 episodes - episode_reward: 8.612 [1.000, 37.000] - loss: 1.415 - mean_squared_error: 71.007 - mean_q: 9.210\n",
      "\n",
      "Interval 31 (30000 steps performed)\n",
      "1000/1000 [==============================] - 38s 38ms/step - reward: 0.4340\n",
      "54 episodes - episode_reward: 8.000 [1.000, 18.000] - loss: 1.562 - mean_squared_error: 70.600 - mean_q: 9.122\n",
      "\n",
      "Interval 32 (31000 steps performed)\n",
      "1000/1000 [==============================] - 37s 37ms/step - reward: 0.4190\n",
      "45 episodes - episode_reward: 9.400 [4.000, 32.000] - loss: 1.665 - mean_squared_error: 69.236 - mean_q: 8.750\n",
      "\n",
      "Interval 33 (32000 steps performed)\n",
      "1000/1000 [==============================] - 36s 36ms/step - reward: 0.4460\n",
      "48 episodes - episode_reward: 9.271 [3.000, 26.000] - loss: 1.372 - mean_squared_error: 51.958 - mean_q: 7.640\n",
      "\n",
      "Interval 34 (33000 steps performed)\n",
      "1000/1000 [==============================] - 43s 43ms/step - reward: 0.4050\n",
      "42 episodes - episode_reward: 9.738 [2.000, 27.000] - loss: 1.473 - mean_squared_error: 62.662 - mean_q: 8.473\n",
      "\n",
      "Interval 35 (34000 steps performed)\n",
      "1000/1000 [==============================] - 44s 44ms/step - reward: 0.4640\n",
      "45 episodes - episode_reward: 10.244 [1.000, 35.000] - loss: 1.412 - mean_squared_error: 64.530 - mean_q: 8.637\n",
      "\n",
      "Interval 36 (35000 steps performed)\n",
      "1000/1000 [==============================] - 39s 39ms/step - reward: 0.4740\n",
      "50 episodes - episode_reward: 9.280 [3.000, 20.000] - loss: 1.247 - mean_squared_error: 55.975 - mean_q: 8.012\n",
      "\n",
      "Interval 37 (36000 steps performed)\n",
      "1000/1000 [==============================] - 38s 38ms/step - reward: 0.4230\n",
      "50 episodes - episode_reward: 8.740 [2.000, 22.000] - loss: 1.337 - mean_squared_error: 54.015 - mean_q: 7.860\n",
      "\n",
      "Interval 38 (37000 steps performed)\n",
      "1000/1000 [==============================] - 37s 37ms/step - reward: 0.4500\n",
      "55 episodes - episode_reward: 7.764 [2.000, 25.000] - loss: 1.373 - mean_squared_error: 54.219 - mean_q: 8.058\n",
      "\n",
      "Interval 39 (38000 steps performed)\n",
      "1000/1000 [==============================] - 36s 36ms/step - reward: 0.4120\n",
      "55 episodes - episode_reward: 7.855 [2.000, 28.000] - loss: 1.391 - mean_squared_error: 59.231 - mean_q: 8.293\n",
      "\n",
      "Interval 40 (39000 steps performed)\n",
      "1000/1000 [==============================] - 36s 36ms/step - reward: 0.4630\n",
      "done, took 1486.802 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=40000, visualize=False, verbose=1, log_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hist = np.load(\"E:/Projects/Python/piis_rl_pacman/player_rl/saved/16conv/history.npy\")\n",
    "full_hist = np.concatenate((full_hist, np.array(history.history[\"episode_reward\"])))\n",
    "np.save(\"E:/Projects/Python/piis_rl_pacman/player_rl/saved/16conv/history.npy\", np.array(full_hist))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "plt.plot(moving_average(full_hist, 100))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32700bc3807e85cff4169f1494086cde6a49298b14454c6caa68f1dd2006857d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('game.ai': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
